{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAWM3MJOQSyh",
        "outputId": "fbdbdc6f-e2b9-4d43-84c1-125d430735f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.40.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<6,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.40.0-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.40.0 watchdog-5.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaCTXW8dQsfM",
        "outputId": "35bf1678-6dd6-449d-d066-1c8d364a5c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube_transcript_api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2024.8.30)\n",
            "Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: youtube_transcript_api\n",
            "Successfully installed youtube_transcript_api-0.6.2\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube_transcript_api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jDfE4xOQs0B",
        "outputId": "00ab2737-e77c-4e25-a901-69f101f7f2db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoCkE3jf8cYi",
        "outputId": "56892fcb-5fa7-4a94-ca1a-a7441579f4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDGTUBcsSEBj",
        "outputId": "f3b9c89e-69a2-426b-ca9f-2d3af29a548d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4KQOwCXSEI9",
        "outputId": "d33e8a83-f367-4774-f8ee-9bf312fd24d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uiuedg9XSEQ-",
        "outputId": "d13c2a71-0de2-4a0b-9a1c-580062328e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install rouge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiMyqSjvkNfK",
        "outputId": "b120f0c9-7842-406c-af1f-ccf53d40aa7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (10.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uSY2si9j2tN",
        "outputId": "cb6fcf3b-5990-474b-bdab-3c96db9995e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gtts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F2o-HuE2POB",
        "outputId": "6ee2b3ad-688e-49ea-9f74-f27740ac95bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.3-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2024.8.30)\n",
            "Downloading gTTS-2.5.3-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gtts\n",
            "Successfully installed gtts-2.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFHPuxy4QSts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6373d9a-4c17-4352-f2ee-d0e5aaad920a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting myapp.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile myapp.py\n",
        "from gtts import gTTS\n",
        "import streamlit as st\n",
        "import os\n",
        "from googleapiclient.discovery import build\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from transformers import pipeline, PegasusForConditionalGeneration, AutoTokenizer, MarianMTModel, MarianTokenizer, AutoModelForSeq2SeqLM\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rouge import Rouge\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import torch\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Your code that may trigger warnings\n",
        "\n",
        "# Define functions to perform summarization and evaluation tasks\n",
        "@st.cache_data(show_spinner=False)\n",
        "def summarize(text):\n",
        "    summarization = pipeline('summarization')\n",
        "    summarized_text = summarization(text)\n",
        "    return summarized_text[0]['summary_text']\n",
        "\n",
        "@st.cache_data(show_spinner=False)\n",
        "def pegasus_summarize(text):\n",
        "    model_name = 'google/pegasus-cnn_dailymail'\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "    batch = tokenizer(text, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
        "    translated = model.generate(**batch)\n",
        "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
        "    tgt_text = tgt_text.replace('<n>', '\\n')\n",
        "    return tgt_text\n",
        "\n",
        "def evaluate_cosine_similarity(t1, t2):\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectorized_text = vectorizer.fit_transform([t1, t2])\n",
        "    similarity_score = cosine_similarity(vectorized_text)[0][1]\n",
        "    return similarity_score\n",
        "\n",
        "def evaluate_f1_score(t1, t2):\n",
        "    ref_tokens = nltk.word_tokenize(t2.lower())\n",
        "    gen_tokens = nltk.word_tokenize(t1.lower())\n",
        "    tp = len(set(ref_tokens) & set(gen_tokens))\n",
        "    fp = len(gen_tokens) - tp\n",
        "    fn = len(ref_tokens) - tp\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
        "    return f1_score\n",
        "\n",
        "def evaluate_rouge_score(t1, t2):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(t1, t2)\n",
        "    return scores\n",
        "\n",
        "def evaluate_polarity_score(text):\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    polarity = scores['compound']\n",
        "    return polarity\n",
        "\n",
        "def text_to_speech_gtts(text, lang='en', slow=False, output_file='output.mp3'):\n",
        "    try:\n",
        "        tts = gTTS(text=text, lang=lang, slow=slow)\n",
        "        tts.save(output_file)\n",
        "        return output_file\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# @st.cache_data(show_spinner=False)\n",
        "# def translate_text(text, src_lang, tgt_lang):\n",
        "#     model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
        "#     tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "#     model = MarianMTModel.from_pretrained(model_name)\n",
        "#     translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))\n",
        "#     tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "#     return tgt_text[0]\n",
        "\n",
        "API_KEY = 'AIzaSyD4JPHwISaNHsdKVSreFTakg2VB2ifKJMo'\n",
        "VIDEO_ID = '0IAPZzGSbME'\n",
        "\n",
        "def get_related_videos(video_id, api_key):\n",
        "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "    video_details = youtube.videos().list(\n",
        "        part=\"snippet\",\n",
        "        id=video_id\n",
        "    ).execute()\n",
        "\n",
        "    if not video_details['items']:\n",
        "        print(\"Video not found.\")\n",
        "        return\n",
        "\n",
        "    channel_id = video_details['items'][0]['snippet']['channelId']\n",
        "\n",
        "    request = youtube.search().list(\n",
        "        part=\"snippet\",\n",
        "        maxResults=10,\n",
        "        type=\"video\",\n",
        "        channelId=channel_id\n",
        "    )\n",
        "    response = request.execute()\n",
        "    related_videos = []\n",
        "    for item in response['items']:\n",
        "        video_title = item['snippet']['title']\n",
        "        video_id = item['id']['videoId']\n",
        "        video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "        related_videos.append({\"title\": video_title, \"url\": video_url})\n",
        "    return related_videos\n",
        "@st.cache_data(show_spinner=False)\n",
        "def translate_text(text, src_lang, tgt_lang):\n",
        "\n",
        "    model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))\n",
        "    tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "    return tgt_text[0]\n",
        "\n",
        "\n",
        "# Define Streamlit app\n",
        "def app():\n",
        "    st.title('YouTube Video Transcription Summarization')\n",
        "\n",
        "    # Get input from user\n",
        "    video_url = st.text_input('Enter YouTube video link:')\n",
        "    if not video_url:\n",
        "        st.warning('Please enter a valid YouTube video link')\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with st.spinner(\"Hang tight, we're working on it!\"):\n",
        "          # Get transcript from YouTube video\n",
        "          video_id = video_url.split(\"=\")[1]\n",
        "          transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "          result = \"\"\n",
        "          for i in transcript:\n",
        "              result += ' ' + i['text']\n",
        "              result = ' '.join(result.split()[:512])\n",
        "\n",
        "\n",
        "          # Summarize the transcript\n",
        "          summarized_text = summarize(result)\n",
        "\n",
        "           # Use Pegasus to summarize the transcript\n",
        "          pegasus_summarized_text = pegasus_summarize(result)\n",
        "\n",
        "           # Format the translated text\n",
        "          sentences = pegasus_summarized_text .split('. ')\n",
        "          bullet_points = [f'* {s}' for s in sentences]\n",
        "\n",
        "          # Evaluate similarity between pegasus_summarized_text and summarized transcript\n",
        "          similarity_score = evaluate_cosine_similarity(pegasus_summarized_text , summarized_text)\n",
        "\n",
        "          # Evaluate F1 score between pegasus_summarized_text and summarized transcript\n",
        "          f1_score = evaluate_f1_score(pegasus_summarized_text , summarized_text)\n",
        "\n",
        "          # Evaluate Rouge score between pegasus_summarized_text and summarized transcript\n",
        "          rouge_score = evaluate_rouge_score(pegasus_summarized_text , summarized_text)\n",
        "\n",
        "          # Evaluate polarity score of pegasus_summarized_text\n",
        "          polarity_score = evaluate_polarity_score(pegasus_summarized_text)\n",
        "\n",
        "        # Display results to user\n",
        "        st.header('Summary of Transcript')\n",
        "        st.write('\\n'.join(bullet_points))\n",
        "\n",
        "        st.write('')\n",
        "        st.write('')\n",
        "\n",
        "        # Define stop words\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        # Add custom stop words\n",
        "        custom_stop_words = ['video', 'watch', 'transcript','like','share','subscribe']\n",
        "        stop_words.update(custom_stop_words)\n",
        "\n",
        "        wc = WordCloud(width=800, height=400, background_color=\"white\", stopwords = stop_words, colormap=\"Dark2\").generate(summarized_text)\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        plt.imshow(wc, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.tight_layout(pad=0)\n",
        "        # st.set_option('deprecation.showPyplotGlobalUse', False)\n",
        "        import warnings\n",
        "        warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "        st.pyplot()\n",
        "        st.write('')\n",
        "        st.write('')\n",
        "        #Text to speech conversion\n",
        "        st.title(\"Voice Note\")\n",
        "\n",
        "        lang = st.selectbox(\"Select language:\", ['en', 'es', 'fr', 'de','hi'])\n",
        "        # slow = st.checkbox(\"Slow speech\")\n",
        "\n",
        "        # if st.button(\"Convert\"):\n",
        "        #   output_file = text_to_speech_gtts(summarized_text, lang, slow)\n",
        "        #   if output_file:\n",
        "        #     st.audio(output_file, format='audio/mp3')\n",
        "        if st.button(\"Convert\"):\n",
        "            src_lang = 'en'  # Source language of summarized text\n",
        "            if lang != 'en':\n",
        "                if lang in ['es', 'fr', 'de', 'hi']:\n",
        "                    tgt_lang_map = {\n",
        "                        'es': 'es',\n",
        "                        'fr': 'fr',\n",
        "                        'de': 'de',\n",
        "                        'hi': 'hi'\n",
        "                    }\n",
        "                    translated_text = translate_text(summarized_text, src_lang, tgt_lang_map[lang])\n",
        "                else:\n",
        "                    st.warning(f\"Translation for {lang} is not supported. Using English instead.\")\n",
        "                    translated_text = summarized_text\n",
        "            else:\n",
        "                translated_text = summarized_text\n",
        "\n",
        "            output_file = text_to_speech_gtts(translated_text, lang, False)\n",
        "            if output_file:\n",
        "                st.audio(output_file, format='audio/mp3')\n",
        "        st.write('')\n",
        "        st.write('')\n",
        "\n",
        "        #realted videos\n",
        "        st.title('YouTube Related Videos Finder')\n",
        "        if st.button('Get Related Videos'):\n",
        "          with st.spinner('Fetching related videos...'):\n",
        "            related_videos = get_related_videos(video_id, API_KEY)\n",
        "\n",
        "            if isinstance(related_videos, str):\n",
        "                st.error(related_videos)\n",
        "            else:\n",
        "                st.write(f\"Related videos : \")\n",
        "                for video in related_videos:\n",
        "                    st.write(f\"[{video['title']}]({video['url']})\")\n",
        "\n",
        "        st.write('')\n",
        "        st.write('')\n",
        "\n",
        "        # Display evaluation metrics\n",
        "        st.header('Evaluation Metrics')\n",
        "        st.write('Cosine Similarity Score:', similarity_score)\n",
        "        st.write('F1 Score:', f1_score)\n",
        "        st.write('Rouge Score:', rouge_score[0]['rouge-1']['f'])\n",
        "        st.write('Polarity Score:', polarity_score)\n",
        "\n",
        "        # Calculate the compression ratio\n",
        "        compressed_length = len(pegasus_summarized_text)\n",
        "        original_length = len(result)\n",
        "        compression_ratio = round((compressed_length / original_length) * 100, 2)\n",
        "\n",
        "        st.write(f'**Compression ratio:** {compression_ratio:.2f}%')\n",
        "\n",
        "    except Exception as e:\n",
        "        st.warning('Error occurred: ' + str(e))\n",
        "\n",
        "# Run Streamlit app\n",
        "app()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcJcqaXvquNu",
        "outputId": "16468e02-2df3-4305-8c18-80289928e809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############\n",
            "IPv4\n",
            "34.127.65.200\n",
            "############\n"
          ]
        }
      ],
      "source": [
        "print(\"############\")\n",
        "print(\"IPv4\")\n",
        "!curl https://ipv4.icanhazip.com/\n",
        "print(\"############\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9NIyfzPQ1Hb",
        "outputId": "f7dada1c-20f9-4457-dd08-7819255e5e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.127.65.200:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://tough-pens-return.loca.lt\n",
            "2024-11-07 08:41:50.397508: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-07 08:41:50.421577: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-07 08:41:50.428984: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-07 08:41:50.446889: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-07 08:41:51.564883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "2024-11-07 08:41:54.797 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2024-11-07 08:42:55.074 \n",
            "Calling `st.pyplot()` without providing a figure argument has been deprecated\n",
            "and will be removed in a later version as it requires the use of Matplotlib's\n",
            "global figure object, which is not thread-safe.\n",
            "\n",
            "To future-proof this code, you should pass in a figure as shown below:\n",
            "\n",
            "```python\n",
            "fig, ax = plt.subplots()\n",
            "ax.scatter([1, 2, 3], [1, 2, 3])\n",
            "# other plotting actions...\n",
            "st.pyplot(fig)\n",
            "```\n",
            "\n",
            "If you have a specific use case that requires this functionality, please let us\n",
            "know via [issue on Github](https://github.com/streamlit/streamlit/issues).\n",
            "\n",
            "2024-11-07 08:42:56.820 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "2024-11-07 08:43:14.826 \n",
            "Calling `st.pyplot()` without providing a figure argument has been deprecated\n",
            "and will be removed in a later version as it requires the use of Matplotlib's\n",
            "global figure object, which is not thread-safe.\n",
            "\n",
            "To future-proof this code, you should pass in a figure as shown below:\n",
            "\n",
            "```python\n",
            "fig, ax = plt.subplots()\n",
            "ax.scatter([1, 2, 3], [1, 2, 3])\n",
            "# other plotting actions...\n",
            "st.pyplot(fig)\n",
            "```\n",
            "\n",
            "If you have a specific use case that requires this functionality, please let us\n",
            "know via [issue on Github](https://github.com/streamlit/streamlit/issues).\n",
            "\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "2024-11-07 08:43:16.940 \n",
            "Calling `st.pyplot()` without providing a figure argument has been deprecated\n",
            "and will be removed in a later version as it requires the use of Matplotlib's\n",
            "global figure object, which is not thread-safe.\n",
            "\n",
            "To future-proof this code, you should pass in a figure as shown below:\n",
            "\n",
            "```python\n",
            "fig, ax = plt.subplots()\n",
            "ax.scatter([1, 2, 3], [1, 2, 3])\n",
            "# other plotting actions...\n",
            "st.pyplot(fig)\n",
            "```\n",
            "\n",
            "If you have a specific use case that requires this functionality, please let us\n",
            "know via [issue on Github](https://github.com/streamlit/streamlit/issues).\n",
            "\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "2024-11-07 08:43:26.031 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "2024-11-07 08:43:33.381 \n",
            "Calling `st.pyplot()` without providing a figure argument has been deprecated\n",
            "and will be removed in a later version as it requires the use of Matplotlib's\n",
            "global figure object, which is not thread-safe.\n",
            "\n",
            "To future-proof this code, you should pass in a figure as shown below:\n",
            "\n",
            "```python\n",
            "fig, ax = plt.subplots()\n",
            "ax.scatter([1, 2, 3], [1, 2, 3])\n",
            "# other plotting actions...\n",
            "st.pyplot(fig)\n",
            "```\n",
            "\n",
            "If you have a specific use case that requires this functionality, please let us\n",
            "know via [issue on Github](https://github.com/streamlit/streamlit/issues).\n",
            "\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "2024-11-07 08:43:40.979 \n",
            "Calling `st.pyplot()` without providing a figure argument has been deprecated\n",
            "and will be removed in a later version as it requires the use of Matplotlib's\n",
            "global figure object, which is not thread-safe.\n",
            "\n",
            "To future-proof this code, you should pass in a figure as shown below:\n",
            "\n",
            "```python\n",
            "fig, ax = plt.subplots()\n",
            "ax.scatter([1, 2, 3], [1, 2, 3])\n",
            "# other plotting actions...\n",
            "st.pyplot(fig)\n",
            "```\n",
            "\n",
            "If you have a specific use case that requires this functionality, please let us\n",
            "know via [issue on Github](https://github.com/streamlit/streamlit/issues).\n",
            "\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "2024-11-07 08:44:15.566 \n",
            "Calling `st.pyplot()` without providing a figure argument has been deprecated\n",
            "and will be removed in a later version as it requires the use of Matplotlib's\n",
            "global figure object, which is not thread-safe.\n",
            "\n",
            "To future-proof this code, you should pass in a figure as shown below:\n",
            "\n",
            "```python\n",
            "fig, ax = plt.subplots()\n",
            "ax.scatter([1, 2, 3], [1, 2, 3])\n",
            "# other plotting actions...\n",
            "st.pyplot(fig)\n",
            "```\n",
            "\n",
            "If you have a specific use case that requires this functionality, please let us\n",
            "know via [issue on Github](https://github.com/streamlit/streamlit/issues).\n",
            "\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "2024-11-07 08:44:25.074 \n",
            "Calling `st.pyplot()` without providing a figure argument has been deprecated\n",
            "and will be removed in a later version as it requires the use of Matplotlib's\n",
            "global figure object, which is not thread-safe.\n",
            "\n",
            "To future-proof this code, you should pass in a figure as shown below:\n",
            "\n",
            "```python\n",
            "fig, ax = plt.subplots()\n",
            "ax.scatter([1, 2, 3], [1, 2, 3])\n",
            "# other plotting actions...\n",
            "st.pyplot(fig)\n",
            "```\n",
            "\n",
            "If you have a specific use case that requires this functionality, please let us\n",
            "know via [issue on Github](https://github.com/streamlit/streamlit/issues).\n",
            "\n",
            "tokenizer_config.json: 100% 44.0/44.0 [00:00<00:00, 294kB/s]\n",
            "config.json: 100% 1.39k/1.39k [00:00<00:00, 7.47MB/s]\n",
            "source.spm: 100% 812k/812k [00:00<00:00, 3.89MB/s]\n",
            "target.spm: 100% 1.07M/1.07M [00:00<00:00, 6.67MB/s]\n",
            "vocab.json: 100% 2.10M/2.10M [00:00<00:00, 13.6MB/s]\n",
            "pytorch_model.bin: 100% 306M/306M [00:03<00:00, 84.4MB/s]\n",
            "generation_config.json: 100% 293/293 [00:00<00:00, 1.61MB/s]\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "2024-11-07 08:46:52.304 \n",
            "Calling `st.pyplot()` without providing a figure argument has been deprecated\n",
            "and will be removed in a later version as it requires the use of Matplotlib's\n",
            "global figure object, which is not thread-safe.\n",
            "\n",
            "To future-proof this code, you should pass in a figure as shown below:\n",
            "\n",
            "```python\n",
            "fig, ax = plt.subplots()\n",
            "ax.scatter([1, 2, 3], [1, 2, 3])\n",
            "# other plotting actions...\n",
            "st.pyplot(fig)\n",
            "```\n",
            "\n",
            "If you have a specific use case that requires this functionality, please let us\n",
            "know via [issue on Github](https://github.com/streamlit/streamlit/issues).\n",
            "\n",
            "2024-11-07 08:46:54.342 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n"
          ]
        }
      ],
      "source": [
        "!streamlit run myapp.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCSzlu5HrScP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}